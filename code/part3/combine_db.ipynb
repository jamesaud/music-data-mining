{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from typing import List\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer \n",
    "from IPython.display import clear_output\n",
    "import tqdm\n",
    "\n",
    "\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "stemmer = SnowballStemmer('english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean(word):\n",
    "    chars = [char for char in word if char not in string.punctuation]  # Remove Punctuation\n",
    "    word = \"\".join(chars)\n",
    "    word = word.lower()   # Lowercase\n",
    "    return word    \n",
    "\n",
    "\n",
    "def tokenize_lyrics(lyrics: str):\n",
    "    \"\"\" Takes lyrics in string format, returns list of words \"\"\"\n",
    "    replacement_patterns = (r'\\[.*?\\]',    # Between Brackets\n",
    "                           )    \n",
    "    lyrics_clean = re.sub('|'.join(replacement_patterns), '', lyrics)\n",
    "    tokenizer = WhitespaceTokenizer()\n",
    "    tokens = tokenizer.tokenize(lyrics_clean)\n",
    "    \n",
    "    # Split words with hyphens\n",
    "    for word in tokens:\n",
    "        if '-' in word:\n",
    "            i = tokens.index(word)\n",
    "            tokens = tokens[:i] + word.split('-') + tokens[i+1:]\n",
    "        \n",
    "    tokens = filter(lambda token: token != '', tokens)\n",
    "    return list(tokens)\n",
    "\n",
    "\n",
    "def is_stopword(word: str):\n",
    "    return word in STOPWORDS\n",
    "\n",
    "\n",
    "def tokenize(lyrics, stopwords=True, lemandstem=True):\n",
    "    tokens = tokenize_lyrics(lyrics)\n",
    "    tokens = map(clean, tokens)\n",
    "    if stopwords:\n",
    "        tokens = [word for word in tokens if not is_stopword(word)]\n",
    "    if lemandstem:\n",
    "        tokens = map(lemmatizer.lemmatize, tokens)\n",
    "        tokens = map(stemmer.stem, tokens)\n",
    "        tokens = map(lemmatizer.lemmatize, tokens)\n",
    "    tokens = list(tokens)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def add_tokenization_to_track(track):\n",
    "    track['raw_lyrics'] = track['lyrics']['result']['track']['text']\n",
    "    track['tokens'] = tokenize(track['lyrics']['result']['track']['text'])\n",
    "    return track\n",
    "\n",
    "def lyric_emotion(song):\n",
    "    lyrics = song['lyrics']['result']['track']['text']\n",
    "    lyrics = lyrics.split(\"\\n\")\n",
    "    \n",
    "    compound_score = 0\n",
    "    for sentence in lyrics:\n",
    "        sid = SentimentIntensityAnalyzer()\n",
    "        ss = sid.polarity_scores(sentence)\n",
    "        compound_score += ss['compound']\n",
    "    \n",
    "    return compound_score / len(lyrics)   # AVG compound score\n",
    "\n",
    "\n",
    "def update_song_emotion(song):\n",
    "    song['emotion'] = lyric_emotion(song)\n",
    "    return song\n",
    "\n",
    "def parallel_update_song_emotions(songs):\n",
    "    updated = []\n",
    "    with Pool() as pool:\n",
    "        for song in tqdm.tqdm(pool.imap_unordered(update_song_emotion, songs), total=len(songs)):\n",
    "            clear_output(wait=True)\n",
    "            updated.append(song)\n",
    "    return updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pop-songs': 3374,\n",
       " 'latin-songs': 4126,\n",
       " 'country-songs': 9722,\n",
       " 'rock-songs': 4560,\n",
       " 'jazz-songs': 1596,\n",
       " 'christian-songs': 4480,\n",
       " 'rap-song': 5914}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "from pprint import pprint\n",
    "from typing import List\n",
    "from functools import reduce\n",
    "from operator import add\n",
    "from multiprocessing import Pool\n",
    "from operator import add\n",
    "\n",
    "# DB CONFIG\n",
    "client = MongoClient('mongo', 27017)\n",
    "db = client.music_db\n",
    "COLLECTIONS =   ['pop-songs',\n",
    "                 'latin-songs',\n",
    "                 'country-songs',\n",
    "                 'rock-songs',\n",
    "                 'jazz-songs',\n",
    "                 'christian-songs',\n",
    "                 'rap-song']\n",
    "\n",
    "def has_spotify(song):\n",
    "    return song.get('spotify', {}).get('audio_features')\n",
    "\n",
    "def has_lyrics(song):\n",
    "    if not song.get('lyrics'):\n",
    "        return False\n",
    "    \n",
    "    if song.get('lyrics') == 'null':\n",
    "        return False\n",
    "\n",
    "    if song.get('lyrics').get('error'):\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "def update_collections(collection_names, require_spotify=True, require_lyrics=True):\n",
    "    all_songs = []    \n",
    "    for name in collection_names:\n",
    "        collection = db[name]\n",
    "        for i, song in enumerate(collection.find()):\n",
    "            song['genre'] = name\n",
    "            if require_spotify and has_spotify(song):\n",
    "                song.update(song['spotify']['audio_features'])\n",
    "                \n",
    "            if require_lyrics and has_lyrics(song):\n",
    "                add_tokenization_to_track(song)\n",
    "                print(\"Added\", i, end=\" \")\n",
    "                clear_output(wait=True)\n",
    "                \n",
    "            if (has_spotify(song) if require_spotify else True) and (has_lyrics(song) if require_lyrics else True):\n",
    "                all_songs.append(song)\n",
    "            \n",
    "    return all_songs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dict([(collection, len(list(db[collection].find()))) for collection in COLLECTIONS])\n",
    "        \n",
    "#songs = update_collections(COLLECTIONS, False, False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|██████████| 356/356 [00:41<00:00,  8.54it/s]\u001b[A\u001b[A"
     ]
    }
   ],
   "source": [
    "songs = parallel_update_song_emotions(songs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = db['all-songs']\n",
    "db.drop_collection(\"all-songs\")\n",
    "\n",
    "def insert_all_songs(songs, collection):\n",
    "    collection.insert_many(songs)\n",
    "\n",
    "insert_all_songs(songs, collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
